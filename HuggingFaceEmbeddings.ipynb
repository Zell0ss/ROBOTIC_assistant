{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "FAQ_CHROMA_PATH=\"chroma_data\"\n",
    "\n",
    "#load myiam\n",
    "with open(\"documents/myiam.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Chunk the text\n",
    "splitter = MarkdownTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# Merge headers with content\n",
    "documents = []\n",
    "for chunk in chunks: \n",
    "    header = chunk.split('\\n')[0]  # Assuming header is followed by /n\n",
    "    documents.append(Document(page_content=chunk, metadata={\"header\": header, \"category\": \"MyIAM\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load PO\n",
    "with open(\"documents/product_owner.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Chunk the text\n",
    "splitter = MarkdownTextSplitter(chunk_size=1024, chunk_overlap=50)\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# Merge headers with content\n",
    "for chunk in chunks:  \n",
    "    header = chunk.split('\\n')[0]\n",
    "    documents.append(Document(page_content=chunk, metadata={\"header\": header, \"category\": \"PO\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingfaceembeddings downloads the model the first time and then it uses it locally, but we want it in local from the get to go. You need sentence-transformers pip installed to use them.\n",
    "\n",
    "For getting the model locally you can go to \n",
    "- [https://huggingface.co/sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)  Better quality but slower: 1m 32 seconds\n",
    "- [https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)  Fast and lightweight: 4 segs\n",
    "\n",
    "Click \"Files and versions\" tab\n",
    "Download all the files to a local directory WITH GIT LFS \n",
    "```shell\n",
    "./models/all-MiniLM-L6-v2/\n",
    "├── config.json\n",
    "├── pytorch_model.bin\n",
    "├── tokenizer.json\n",
    "├── tokenizer_config.json\n",
    "├── vocab.txt\n",
    "└── modules.json\n",
    "```\n",
    "### with sentence-transformers directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"./models/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create wrapper for LangChain\n",
    "class LocalEmbeddings:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, convert_to_tensor=False).tolist()\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], convert_to_tensor=False)[0].tolist()\n",
    "\n",
    "embeddings = LocalEmbeddings(model)\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=FAQ_CHROMA_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize local embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    # model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  \n",
    "    model_name=\"./models/all-MiniLM-L6-v2\", #local path\n",
    "    model_kwargs={'device': 'cpu'}  # or 'cuda' if you have GPU \n",
    ")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=FAQ_CHROMA_PATH\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
